{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pre_data2 = pd.read_csv(\"pre_dataset2.csv\")\n",
    "\n",
    "text_column = 'post_body_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Ensure the column is string type to avoid errors with non-text data\n",
    "pre_data2[text_column] = pre_data2[text_column].astype(str)\n",
    "\n",
    "# Remove URLs \n",
    "pre_data2[text_column] = pre_data2[text_column].str.replace(r'https?://\\S+|www\\.\\S+', '', regex=True)\n",
    "\n",
    "# Remove emojis using a comprehensive regex pattern\n",
    "emoji_pattern = re.compile(\n",
    "    \"[\"\n",
    "    \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # Alchemical symbols\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # Flag (iOS)\n",
    "    \"\\U00002702-\\U000027B0\"\n",
    "    \"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\",\n",
    "    flags=re.UNICODE,\n",
    ")\n",
    "pre_data2[text_column] = pre_data2[text_column].str.replace(emoji_pattern, '', regex=True)\n",
    "\n",
    "# Remove hashtags and the entire word associated\n",
    "pre_data2[text_column] = pre_data2[text_column].str.replace(r'#\\w+', '', regex=True)\n",
    "\n",
    "# Clean up extra whitespace\n",
    "pre_data2[text_column] = pre_data2[text_column].str.strip()  # Remove leading/trailing spaces\n",
    "pre_data2[text_column] = pre_data2[text_column].str.replace(r'\\s+', ' ', regex=True) # Replace multiple spaces with a single one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all repeated posts \n",
    "pre_data2.drop_duplicates(subset=[text_column], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  published_at\n",
      "0   2020-11-24\n",
      "1   2020-11-24\n",
      "2   2020-11-24\n",
      "3   2020-11-24\n",
      "5   2020-11-23\n"
     ]
    }
   ],
   "source": [
    "# change published_at dates\n",
    "pre_data2['published_at'] = pd.to_datetime(pre_data2['published_at'])\n",
    "\n",
    "# Extract just the date part (YYYY-MM-DD)\n",
    "pre_data2['published_at'] = pre_data2['published_at'].dt.date\n",
    "\n",
    "print(pre_data2[['published_at']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new CSV file \n",
    "pre_data2.to_csv(\"cleaned_dataset1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          PostId published_at  \\\n",
      "0      430137083   2020-11-24   \n",
      "1      440975315   2020-11-24   \n",
      "2      440975410   2020-11-24   \n",
      "3      232168406   2020-11-24   \n",
      "4      154357285   2020-11-23   \n",
      "...          ...          ...   \n",
      "23810  412473029   2019-11-25   \n",
      "23811  476186768   2019-11-25   \n",
      "23812  487272409   2019-11-25   \n",
      "23813  375101593   2019-11-25   \n",
      "23814   82156817   2019-11-25   \n",
      "\n",
      "                                          post_body_text  \n",
      "0      BCW Pittsburgh is proud to sponsor the Black E...  \n",
      "1      RT @KarinesReyes87: Mayor Dinkins set the city...  \n",
      "2      Mayor Dinkins set the city on the right track ...  \n",
      "3      <p>DeRay, Kaya, Sam, and De'Ara dive into the ...  \n",
      "4      President-elect Biden, in choosing Shuwanza Go...  \n",
      "...                                                  ...  \n",
      "23810  How Queen and Slim Pays Homage to Decades of B...  \n",
      "23811  RT @Dlw20161950: HUGE! Black Support for Presi...  \n",
      "23812  Dada a largada de BLACK WEEK de Nanum Coreano....  \n",
      "23813  Overtime Off the Record we talk presidential p...  \n",
      "23814  We had a wonderful time at the Blue Jeans &amp...  \n",
      "\n",
      "[23815 rows x 3 columns]\n",
      "Index(['PostId', 'published_at', 'post_body_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cleaned1 = pd.read_csv(\"cleaned_dataset1.csv\")\n",
    "print(cleaned1)\n",
    "print(cleaned1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a mask for posts containing the keywords.\n",
    "keyword_mask = cleaned1['post_body_text'].str.contains('black|african american', case=False, na=False)\n",
    "\n",
    "# 2. Create a mask for posts that are 20 or more characters long.\n",
    "length_mask = cleaned1['post_body_text'].str.len() >= 50\n",
    "\n",
    "# 3. Apply both masks to filter the DataFrame.\n",
    "cleaned1 = cleaned1[keyword_mask & length_mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing irrelevant key words\n",
    "\n",
    "irrelevant_keywords = [\n",
    "    \"pepper\",        \n",
    "    \"friday\",       \n",
    "    \"cat\", \n",
    "    \"cats\"          \n",
    "    \"dress\",        \n",
    "    \"coffee\",       \n",
    "    \"tea\",           \n",
    "    \"hole\",         \n",
    "    \"box\",           \n",
    "    \"market\",        \n",
    "    \"licorice\",     \n",
    "    \"ink\",           \n",
    "    \"color\",         \n",
    "    \"shoes\",         \n",
    "    \"screen\",        \n",
    "    \"belt\",           \n",
    "    \"plague\",\n",
    "    \"hair\", \n",
    "    \"sheep\",\n",
    "    \"magic\",\n",
    "    \"out\",\n",
    "    \"list\", \n",
    "    \"window\",\n",
    "    \"shirt\",\n",
    "    \"photo\"\n",
    "]\n",
    "\n",
    "def is_irrelevant(post_body_text, irrelevant_terms):\n",
    "    \"\"\"\n",
    "    Checks if a given text contains any of the irrelevant terms,\n",
    "    indicating a non-racial usage of \"black\".\n",
    "    The check is case-insensitive and looks for whole words.\n",
    "    \"\"\"\n",
    "    if pd.isna(post_body_text): \n",
    "        return False\n",
    "    post_body_text_lower = str(post_body_text).lower() \n",
    "    for term in irrelevant_terms:\n",
    "        if re.search(r'\\b' + re.escape(term) + r'\\b', post_body_text_lower):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cleaned Data (Irrelevant Posts Removed) ---\n",
      "          PostId published_at  \\\n",
      "0      430137083   2020-11-24   \n",
      "1      440975315   2020-11-24   \n",
      "2      232168406   2020-11-24   \n",
      "3      314080797   2020-11-23   \n",
      "4      475916367   2020-11-21   \n",
      "...          ...          ...   \n",
      "16784  412473029   2019-11-25   \n",
      "16785  476186768   2019-11-25   \n",
      "16786  487272409   2019-11-25   \n",
      "16787  375101593   2019-11-25   \n",
      "16788   82156817   2019-11-25   \n",
      "\n",
      "                                          post_body_text  \n",
      "0      BCW Pittsburgh is proud to sponsor the Black E...  \n",
      "1      RT @KarinesReyes87: Mayor Dinkins set the city...  \n",
      "2      <p>DeRay, Kaya, Sam, and De'Ara dive into the ...  \n",
      "3      FOR IMMEDIATE RELEASE PA AUDITOR GENERAL-ELECT...  \n",
      "4      My question is how'd the African American vote...  \n",
      "...                                                  ...  \n",
      "16784  How Queen and Slim Pays Homage to Decades of B...  \n",
      "16785  RT @Dlw20161950: HUGE! Black Support for Presi...  \n",
      "16786  Dada a largada de BLACK WEEK de Nanum Coreano....  \n",
      "16787  Overtime Off the Record we talk presidential p...  \n",
      "16788  We had a wonderful time at the Blue Jeans &amp...  \n",
      "\n",
      "[16789 rows x 3 columns]\n",
      "\n",
      "Number of posts after cleaning: 16789\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'text' column is clean \n",
    "cleaned1['post_body_text'] = cleaned1['post_body_text'].astype(str)\n",
    "\n",
    "# Create a boolean mask: True for relevant posts, False for irrelevant ones.\n",
    "relevant_mask = ~cleaned1['post_body_text'].apply(lambda x: is_irrelevant(x, irrelevant_keywords))\n",
    "\n",
    "# Filter the DataFrame using the mask\n",
    "cleaned2 = cleaned1[relevant_mask].copy() # .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "cleaned2 = cleaned2.reset_index(drop=True)\n",
    "\n",
    "print(\"--- Cleaned Data (Irrelevant Posts Removed) ---\")\n",
    "print(cleaned2)\n",
    "print(f\"\\nNumber of posts after cleaning: {len(cleaned2)}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned2.to_csv('cleaned_dataset2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing pre and post \n",
    "cleaned2 = pd.read_csv(\"cleaned_dataset2.csv\")\n",
    "\n",
    "cleaned2['published_at'] = pd.to_datetime(cleaned2['published_at'], errors='coerce')\n",
    "split_date = pd.Timestamp(\"2020-05-25\")\n",
    "\n",
    "# Filter into pre-event and post-event datasets\n",
    "pre_cleaned2 = cleaned2[cleaned2['published_at'] < split_date]\n",
    "post_cleaned2 = cleaned2[cleaned2['published_at'] >= split_date]\n",
    "\n",
    "\n",
    "pre_cleaned2.to_csv(\"pre_blm_cleaned2.csv\", index=False)\n",
    "post_cleaned2.to_csv(\"post_blm_cleaned2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         PostId published_at  \\\n",
      "0     476721818   2020-05-24   \n",
      "1     476721844   2020-05-24   \n",
      "2     313534965   2020-05-23   \n",
      "3       1302491   2020-05-23   \n",
      "4     475545296   2020-05-22   \n",
      "...         ...          ...   \n",
      "3742  412473029   2019-11-25   \n",
      "3743  476186768   2019-11-25   \n",
      "3744  487272409   2019-11-25   \n",
      "3745  375101593   2019-11-25   \n",
      "3746   82156817   2019-11-25   \n",
      "\n",
      "                                         post_body_text  \n",
      "0     RT @DonaldDynasty: The Democrats like to prete...  \n",
      "1     The Democrats like to pretend to be the party ...  \n",
      "2     RT @SecretaryCarson: It is disheartening to se...  \n",
      "3     Trump 2020 senior advisor Katrina Pierson comm...  \n",
      "4     The Story of Black Appalachia is rarely told.....  \n",
      "...                                                 ...  \n",
      "3742  How Queen and Slim Pays Homage to Decades of B...  \n",
      "3743  RT @Dlw20161950: HUGE! Black Support for Presi...  \n",
      "3744  Dada a largada de BLACK WEEK de Nanum Coreano....  \n",
      "3745  Overtime Off the Record we talk presidential p...  \n",
      "3746  We had a wonderful time at the Blue Jeans &amp...  \n",
      "\n",
      "[3747 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "pre_cleaned2 = pd.read_csv(\"pre_blm_cleaned2.csv\")\n",
    "print(pre_cleaned2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          PostId published_at  \\\n",
      "0      430137083   2020-11-24   \n",
      "1      440975315   2020-11-24   \n",
      "2      232168406   2020-11-24   \n",
      "3      314080797   2020-11-23   \n",
      "4      475916367   2020-11-21   \n",
      "...          ...          ...   \n",
      "13037  269941062   2020-05-25   \n",
      "13038  153189948   2020-05-25   \n",
      "13039  242757680   2020-05-25   \n",
      "13040  313329734   2020-05-25   \n",
      "13041   73118219   2020-05-25   \n",
      "\n",
      "                                          post_body_text  \n",
      "0      BCW Pittsburgh is proud to sponsor the Black E...  \n",
      "1      RT @KarinesReyes87: Mayor Dinkins set the city...  \n",
      "2      <p>DeRay, Kaya, Sam, and De'Ara dive into the ...  \n",
      "3      FOR IMMEDIATE RELEASE PA AUDITOR GENERAL-ELECT...  \n",
      "4      My question is how'd the African American vote...  \n",
      "...                                                  ...  \n",
      "13037  The deliberate destruction of Black Afrikans c...  \n",
      "13038  This plant lover is on a quest to show Black m...  \n",
      "13039  RT @NCGOP: A put .@JoeBiden on BLAST for 'You ...  \n",
      "13040  Black Americans are dying of Covid-19 at three...  \n",
      "13041  Son of the Dust- Black Casino and the Ghost Th...  \n",
      "\n",
      "[13042 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "post_cleaned2 = pd.read_csv(\"post_blm_cleaned2.csv\")\n",
    "print(post_cleaned2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Take a random sample of 10,000 posts for the post_blm dataset as it is too large\n",
    "random_10k = post_cleaned2 .sample(n=10000, random_state=42)  \n",
    "\n",
    "random_10k.to_csv(\"post_blm_cleaned2_10k.csv\", index=False)\n",
    "\n",
    "print(f\"Sample size: {len(random_10k)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
